<!-- vscode-markdown-toc -->
* 1. [基础篇](#)
	* 1.1. [通用设计方法](#-1)
	* 1.2. [分层架构](#-1)
	* 1.3. [如何提升系统性能](#-1)
	* 1.4. [高可用](#-1)
	* 1.5. [可扩展](#-1)
* 2. [数据库](#-1)

<!-- vscode-markdown-toc-config
	numbering=true
	autoSave=true
	/vscode-markdown-toc-config -->
<!-- /vscode-markdown-toc -->
##  1. <a name=''></a>基础篇

###  1.1. <a name='-1'></a>通用设计方法
在面带高并发大流量的时候，通常采用三类方法去应对：
- Scale-out横向扩展
  - 通过分布式系统将流量分摊到多台机器上，减少服务器压力。
- 缓存
  - 通过缓存热点数据，让数据读取速率更快。
- 异步
  - 将同步更改为异步，减少CPU阻塞时间，让CPU能处理更多请求。


注意：
- 一般在项目初期的时候使用scale-up去提升系统处理性能，后期qps上来后，提升机器配置产生的收益已经远远低于增加机器数量，所以后期都是通过scale-out去提升服务处理能力。


###  1.2. <a name='-1'></a>分层架构
架构分层可以让每一层专门处理相应的事务，有利于逻辑划分。
例如TCP/IP五层模型、MVC模型等。

分层之间边界需要清晰

例如：
![](typora-user-images/2023-09-21-23-52-29.png)

- 开放接口层：
  - 将Service层方法封装成开放接口，同时进行网关安全控制和流量控制等
- web层
  - 网关层，对请求进行转发、参数校验、权限校验等操作
- Service层（编排层）
  - 业务逻辑层，对原子能力接口进行逻辑上的编排处理
- Manager层
  - 通用业务处理层，提供原子接口
    - 其一，你可以将原先 Service 层的一些通用能力下沉到这一层，比如 与缓存和存储交互策略，中间件的接入；
    - 其二，你也可以在这一层 封装对第三方接口的调用，比如调用支付服务，调用审核服务等。
- Dao层
  - 数据访问层，与底层 MySQL、Oracle、Hbase 等进行数据交互。
- 外部接口或第三方平台：
  - 包括其它部门 RPC 开放接口，基础平台，其它公司的 HTTP 接口。

在这个分层架构中 主要增加了 Manager 层，它与 Service 层的关系是：Manager 层提供原子的服务接口，Service 层负责依据业务逻辑来编排原子接口。

以上面的例子来说，Manager 层提供 创建用户 和 获取用户信息 的接口，而 Service 层负责将这两个接口组装起来。这样就把原先散布在表现层的业务逻辑都统一到了 Service 层，每一层的边界就非常清晰了。


###  1.3. <a name='-1'></a>如何提升系统性能
衡量系统性能的三大指标：高性能、高可用、可扩展

性能都必须建立在高并发的基础上。

**性能优化原则**
- 性能优化是问题导向
- 遵循二八原则
  - 花20%的时间解决80%的问题
- 需要数据支撑：时刻了解优化效果
- 过程是持续的


**性能优化度量指标**
- 平均值
  - 敏感度较差，无法反应真实状况，比如有个请求的时间特别长，会把所有请求的时间都拉下来
- 最大值
  - 过于敏感，问题类似于平均值
- 分位值

**高并发下的性能优化**
- 提高系统的处理核心数
  - 增加系统并行处理能力
- 减少单次任务响应时间
  - 单位时间内能处理更多请求
  - CPU密集型
    - 选用更高效的算法或者减少运算次数就是这类系统重要的优化手段
  - IO密集型
    - 监控和工具发现

###  1.4. <a name='-1'></a>高可用
指的是系统具备较高的无故障运行能力

**度量**

与之相关的概念是： MTBF 和 MTTR。

**MTBF（Mean Time Between Failure）** 是平均故障间隔的意思，代表两次故障的间隔时间，也就是系统正常运转的平均时间。这个时间越长，系统稳定性越高。

**MTTR（Mean Time To Repair）** 表示故障的平均恢复时间，也可以理解为平均故障时间。这个值越小，故障对于用户的影响越小。

可用性与 MTBF 和 MTTR 的值息息相关，我们可以用下面的公式表示它们之间的关系：

``Availability = MTBF / (MTBF + MTTR)``

高可用的系统需要从系统设计和系统运维这两方面来保障：

**系统设计**

包含故障转移、超时控制、限流、降级等

- failover故障转移
  - 对等节点中，也就是分布式节点中，直接进行流量转移即可
  - 非对等节点中，也就是系统中存在主备节点
- 超时控制
  - 收集系统之间的调用日志，统计响应时间
  - 如果请求超过设定时间，则让请求失败。
- 限流
  - 限制单位时间内的qps
- 降级
  - 为了保证核心服务的稳定而牺牲非核心服务的做法。

**系统运维**

包含灰度发布、故障演练等。


###  1.5. <a name='-1'></a>可扩展

指的是通过增加机器的数量来线性提高系统的处理能力，从而承担更高的流量和并发。

无状态的服务和组件更易于扩展，而像Mysql这类有状态的存储就比较难以扩展，因为扩展涉及到大量的数据迁移。

扩展需考虑的因素：
- 服务数量
- 数据库
- 缓存
- 依赖的第三方服务
- 负载均衡
- 交换机带宽等


最重要的思路就是`拆分`

**存储层的扩展性**

存储拆分首先考虑的维度是业务维度，这个就是垂直拆分。

拆分之后，系统就有了用户库、内容库、评论库、点赞库和关系库等，一个挂了之后不影响其他服务。

业务按照垂直拆分运行一段时间后，数据量会增加，那么此时就会需要进行二次拆分，按照数据特征拆分，这个就叫水平拆分，例如对用户库进行分库分表等操作。

当数据库按照业务和数据维度拆分之后，我们 尽量不要使用事务。因为当一个事务中同时更新不同的数据库时，需要使用二阶段提交，来协调所有数据库要么全部更新成功，要么全部更新失败。这个协调的成本会随着资源的扩展不断升高，最终达到无法承受的程度。


**业务层的扩展性**

会从三个维度考虑业务层的拆分方案，它们分别是：业务纬度 ，重要性纬度 和 请求来源纬度。

- 业务纬度：
  
    把相同业务的服务拆分成单独的业务池，比如用户服务单独拆出来部署。

    每个业务依赖独自的数据库资源，不会依赖其它业务的数据库资源，在某个业务需要扩展的时候就会大大减少复杂度。

![](typora-user-images/2023-09-22-00-59-43.png)

- 重要性纬度：

    根据业务接口的重要性，把业务分成核心池和非核心池，优先保证核心池的服务稳定性

    ![](typora-user-images/2023-09-22-01-01-11.png)



##  2. <a name='-1'></a>数据库

### 池化技术
可以减少频繁创建连接引起的性能损耗

1、数据库连接池化

数据库连接池有两个最重要的配置： 最小连接数和最大连接数， 它们控制着从连接池中获取连接的流程：

- 如果当前连接数小于最小连接数，则创建新的连接处理数据库请求；
- 如果连接池中有空闲连接则复用空闲连接；
- 如果空闲池中没有连接并且当前连接数小于最大连接数，则创建新的连接处理请求；
- 如果当前连接数已经大于等于最大连接数，则按照配置中设定的时间（C3P0 的连接池配置是 checkoutTimeout）等待旧的连接可用；
- 如果等待超过了这个设定时间则向用户抛出错误。


2、线程池连接池化

比如java的

- 如果线程池中的线程数少于 coreThreadCount 时，处理新的任务时会创建新的线程；
- 如果线程数大于 coreThreadCount 则把任务丢到一个队列里面，由当前空闲的线程执行；
- 当队列中的任务堆积满了的时候，则继续创建线程，直到达到 maxThreadCount；
- 当线程数达到 maxTheadCount 时还有新的任务提交，那么我们就不得不将它们丢弃了（还得看具体的策略）。

### 读写分离（主从）
两个关键点：
- 主从复制
- 如何屏蔽底层主从架构，让使用者可以像使用单点系统一样使用数据库

#### 主从复制
对MySQL来说，主从复制依赖binlog，主库异步将binlog传输到从库

过程：
- 首先从库在连接到主节点时会创建一个 IO 线程，用以请求主库更新的 binlog，并且把接收到的 binlog 信息写入一个叫做 relay log 的日志文件中（避免直接写入从库耗时较高，造成主从数据延迟）
- 而主库也会创建一个 log dump 线程来发送 binlog 给从库；
- 同时，从库还会创建一个 SQL 线程读取 relay log 中的内容，并且在从库中做回放，最终实现主从的一致性。这是一种比较常见的主从复制方式。


无需太多从库，因为从库每多一个，就会创建一个线程，线程越多，对主库的性能会有影响。最多挂3-5个

**缺点：**

1、主从同步延迟，造成主库写入之后，从库中无法读取的情况

解决方案就是尽量不去从库查询数据：
- 数据冗余：将完整数据发送至队列等，避免二次查询数据
- 缓存：将数据同步写入缓存中，会有缓存与数据不一致的情况
- 直接查询主库： 

#### 如何访问数据库
通过中间件去查询数据，屏蔽主从分离后数据库访问的细节。


### 分库分表
数据库的写入请求大造成的性能和可用性方面的问题？

可以对数据进行分片，分摊读写压力，突破单机的存储瓶颈，也就是分库分表。

数据库分库分表的方式有两种：一种是垂直拆分，另一种是水平拆分。

#### 垂直拆分
将不同业务的表拆分到不同的数据库

垂直拆分的原则一般是按照业务类型来拆分，核心思想是专库专用，将业务耦合度比较高的表拆分到单独的库中

可以暂缓数据库压力，但是如果某个业务是核心业务，数据量增长速度很快，同样会造成数据库瓶颈，需要进行二次拆分，也就是水平拆分


#### 水平拆分
两种方式：
- 根据某个字段hash值进行拆分，将数据放到不同的库中。
- 按照某一个字段的 区间 来拆分，比较常用的是时间字段。

第一种方式无法进行排序
第二种方式可以进行排序，也可以解决热点数据问题，比如热点数据是最近一周或一个月的。


#### 问题：
- 查询都需要带上分区字段
  - 比如根据id分区，查询昵称，这个时候需要对昵称再做分区，就很麻烦，可以对id和昵称建立映射，先查询对应的id，然后再查询。
- 查询次数增多
- 无法进行连表查询


### 发号器
数据库的主键在于全局唯一性，对于单机来说，可以使用自增主键，但是对于主从分离、分库分表来说，就无法使用自增主键，需要一个能生成全局唯一性的主键

很容易想到的是UUID，但UUID是无序的，无法对数据进行排序功能，同时写入数据库也会有相应的性能问题。对于mysql的B+树来说，顺序写比随机写性能要好很多。

其次UUID是32个16进制的数字组成，对于mysql的存储来说也是比较耗费空间



#### 基于Snowflake算法搭建发号器
核心思想是将 64bit 的二进制数字分成若干部分，每一部分都存储有特定含义的数据，比如说时间戳、机器 ID、序列号等等，最终生成全局唯一的有序 ID。它的标准算法是这样的：
![](typora-user-images/2023-09-24-23-45-43.png)

可以根据自身业务对算法进行改造，比如：1 位兼容位恒为 0 + 41 位时间信息 + 6 位 IDC 信息（支持 64 个 IDC）+ 6 位业务信息（支持 64 个业务）+ 10 位自增信息（每毫秒支持 1024 个号）

**算法实现方式：**
- 嵌入到业务代码里，也就是分布在业务服务器中
  - 优点：
    - 无需跨网络调用
  - 缺点：
    - 业务服务器众多，需要更多的机器id位数，由于很难保证机器id的唯一性，所以需要引入第三方组件去保证，比如zk
- 独立的服务部署，也就是发号器服务
  - 缺点：
    - 多一次网络调用
  - 优点：
    - 机器id唯一


雪花算法的缺点是太依赖于时间戳，如果时间戳不准，需要重新校准后再进行发号。


另外，在qps不高的情况下，比如发号器每毫米只发一个，那么id尾数就永远是1，对于分库分表来说会造成数据不均匀的情况，
- 时间戳不记录毫秒而是记录秒，这样在一个时间区间里可以多发出几个号，避免出现分库分表时数据分配不均。

- 生成的序列号的起始号可以做一下随机，这一秒是 21，下一秒是 30，这样就会尽量的均衡了。


### RDS与NoSQL互补

**NoSQL**

- Redis、LevelDB 这样的 KV 存储。这类存储相比于传统的数据库的优势是极高的读写性能，一般对性能有比较高的要求的场景会使用。
- Hbase、Cassandra 这样的 列式存储数据库。这种数据库的特点是数据不像传统数据库以行为单位来存储，而是以列来存储，适用于一些离线数据统计的场景。
- 像 MongoDB、CouchDB、ES 这样的文档型数据库。这种数据库的特点是 Schema Free（模式自由），数据表中的字段可以任意扩展，比如说电商系统中的商品有非常多的字段，并且不同品类的商品的字段也都不尽相同，使用关系型数据库就需要不断增加字段支持，而用文档型数据库就简单很多了。

#### 使用NoSQL提升性能
mysql更新binlog、redolog、undolog都是顺序写，只有更新datafile、索引是随机写，随机写比顺序写性能差了几个等级，所以mysql使用了先写内存然后再批量写磁盘去优化，但是还是会有随机写的情况。

**NoSQL方案**

基于 LSM（（Log-Structured Merge Tree）） 树的存储引擎

数据首先会写入到一个叫做 MemTable 的内存结构中，在 MemTable 中数据是按照写入的 Key 来排序的。为了防止 MemTable 里面的数据因为机器掉电或者重启而丢失，一般会通过写 Write Ahead Log 的方式将数据备份在磁盘上。

MemTable 在累积到一定规模时，它会被刷新生成一个新的文件，我们把这个文件叫做 SSTable（Sorted String Table）。当 SSTable 达到一定数量时，我们会将这些 SSTable 合并，减少文件的数量，因为 SSTable 都是有序的，所以合并的速度也很快。

当从 LSM 树里面读数据时，我们首先从 MemTable 中查找数据，如果数据没有找到，再从 SSTable 中查找数据。因为存储的数据都是有序的，所以查找的效率是很高的，只是因为数据被拆分成多个 SSTable，所以读取的效率会低于 B+ 树索引

![](typora-user-images/2023-09-25-00-07-32.png)



#### 提升扩展性
RDS在数据量大的时候就需要进行分库分表，此时进行数据迁移是一件非常耗时的事情，也很麻烦

此时可以借助NoSQL的特性，
比如MongoDB的特性，
- Replica也就是副本集，自带数据冗余
- Shard，分片，自带分库分表，MongoDB 的 Sharding 特性一般需要三个角色来支持，一个是 Shard Server，它是实际存储数据的节点，是一个独立的 Mongod 进程；二是 Config Server，也是一组 Mongod 进程，主要存储一些元信息，比如说哪些分片存储了哪些数据等；最后是 Route Server，它不实际存储数据，仅仅作为路由使用，它从 Config Server 中获取元信息后，将请求路由到正确的 Shard Server 中。
- 负载均衡，当数据不均匀的时候，自动触发Balancer进行重新分配，需要扩容时，数据会自动迁移到新的节点上，无需手动介入。



NoSQL 数据库中内置的扩展性方面的特性可以让我们不再需要对数据库做分库分表和主从分离，也是对传统数据库一个良好的补充。


