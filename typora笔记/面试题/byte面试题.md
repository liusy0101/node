<!-- vscode-markdown-toc -->
* 1. [后端](#)
	* 1.1. [限流器的设计](#-1)
	* 1.2. [短网址系统](#-1)
	* 1.3. [索引FAQ](#FAQ)
	* 1.4. [计数系统设计](#-1)
	* 1.5. [Redis提供了哪几种持久化方式](#Redis)
	* 1.6. [直播消息服务设计](#-1)
	* 1.7. [问题：](#-1)
	* 1.8. [秒杀系统设计](#-1)
* 2. [算法](#-1)

<!-- vscode-markdown-toc-config
	numbering=true
	autoSave=true
	/vscode-markdown-toc-config -->
<!-- /vscode-markdown-toc -->

##  1. <a name=''></a>后端
###  1.1. <a name='-1'></a>限流器的设计
**请设计一个分布式限流器，实现以下功能：**
- 限流器能work，精确度达到99%以上
- 限流阈值大小可以调节，大到1000w qps，小到100 qps
- 通用限流器，不局限在某个业务领域

**答案**

限流算法：
  - 令牌桶
  - 漏桶
  - 固定窗口计数器
  - 滑动窗口日志
  - 滑动窗口计数器

`令牌桶`

令牌桶是固定数量的容器
- 一方面，按照固定速率向桶中添加令牌，桶满后，多余的令牌会被丢弃
- 另一方面，一个请求消耗一个令牌，如果桶中没有令牌了，那么这个请求就被丢弃。

重要参数有桶的大小和填充速率，同时对于不同的api，可能需要不同的桶去控制不同的速率，在高并发时候，对于这两个参数的调整可能会有比较大的挑战。

`漏桶`

也是固定数量的容器
- 一方面，请求进来时，填充到桶中，桶满后请求会被拒绝
- 另一方面，桶中的请求以固定的速率进行处理

重要参数是桶的大小和处理速率，优点是使用队列易实现，缺点是，面对突发流量时，虽然有的请求已经推到队列中了，但是由于消费的速率是固定的，存在效率问题。

`固定窗口计数器`

把时间划分成固定大小的时间窗口，每个窗口分配一个计数器，接收到一个请求，计数器就加一，一旦计数器达到设定的阈值，新的请求就会被丢弃，直到新的时间窗口，新的计数器

缺点在于如果在时间窗口的边缘出现突发流量时，可能会导致通过的请求数超过阈值。
比如前一个窗口快结束时候流量徒增，后一个窗口开始时候流量依旧保持高位，会导致这个时间点的流量qps大于设置阈值
![](typora-user-images/2023-09-20-04-08-20.png)

固定窗口计数器的优点是，简单易于理解，缺点是，时间窗口的边缘应对流量高峰时，可能会让通过的请求数超过阈值。

`滑动窗口日志`

假如设定1分钟内最多允许2个请求，每个请求都需要记录请求时间，比如保存在 Redis 的 sorted sets 中，保存之后还需要删除掉过时的日志

过时日志如何算：从当前时间点往前推算一个时间窗口，窗口外的数据就是过时日志。

`滑动窗口计数器`

算法与滑动窗口日志类似，滑动窗口把固定的窗口又分成了很多小的窗口单位，比如下图，每个固定窗口的大小为1分钟，又拆分成了6份，每次移动一个小的单位，保证总和不超过阈值。

![](typora-user-images/2023-09-20-04-13-24.png)


**实现**

使用Redis实现高效计数器

利用redis的原子自增和过期淘汰策略
- 限流器的计数存放在redis中，用redis的过期淘汰策略实现限流器的计数的定期更新
- 例如针对 接口A 限流 10000 QPS。redis的key为：“接口A”，value为计数值-每次接口调用Redis
用INC原子自增命令，自增1，并设音过期时间为1s
- 初次调用时，因为redis中该key没有，就直接设置为1，并设音过期时间为1s
- 在这一秒以内的后续调用，每次都自增1-客户端拿到自增后的值如果没有超过限制10000，就放行
  - 如果超过 10000 限制，就不放行，说明超限了
  - 细节实现：为遊免超限后无谓的redis 调用，第一次发现超限时可以记录该值的TTL时间，例如只过去100ms就有1x个请求过来，剩下的900ms就不用请求redis而是直接返回超限即可。不然这种情况会给redis带去额外无谓的流量，例如前面的例子，不做这个细节逻辑的话,redis的请求量是 10W QPS
- 精度可调节。假如限流闻值很大，比如100W，可以把INC自增步进/步长调整大一些，例如100，那么redis的QPS直接降低100倍，为1W QPS

应用：Lyft 是一个开源的限速组件

###  1.2. <a name='-1'></a>短网址系统
1. 分为两个接口
   - 从一个长网址生成一个短网址。需要考虑：同一个长网址，多次创建的短网址是否相同
   - 用户访问短网址时，需要能跳回到原始的长网址
2. 需要考虑跨机房部署问题

**答案**

1. 一开始需要能考虑系统承载容量，例如：
    - 每天100亿访问量
    - 每天生成100ov条短网址记录
2. 然后考虑短网址的生成算法，方案有很多种
    - 最简单实现：白增id实现，这个不可逆，同一个长网址会生成多个短网址
    - hash+序号冲突
    - 使用kv存储双向对应关系，可逆，但存储用量比较大
3. 302跳转问题，附带可以讨论网址访问量计数问题


考虑点：
1. 重定向是用301还是302
   1. 301，代表 永久重定向，也就是说第一次请求拿到长链接后，下次浏览器再去请求短链的话，不会向短网址服务器请求了，而是直接从浏览器的缓存里拿，这样在 server 层面就无法获取到短网址的点击数了，如果这个链接刚好是某个活动的链接，也就无法分析此活动的效果。所以我们一般不采用 301。
   2. 302，代表 临时重定向，也就是说每次去请求短链都会去请求短网址服务器（除非响应中用 Cache-Control 或 Expired 暗示浏览器缓存）,这样就便于 server 统计点击数，所以虽然用 302 会给 server 增加一点压力，但在数据异常重要的今天，这点代码是值得的，所以推荐使用 302！
2. 短链生成的几种方法
   1. 哈希算法
      1. 解决冲突：如果数据存储在mysql中，可以用唯一索引检测，如果有冲突，在长链接上拼接固定字符再进行hash，后续去除固定字符即可。优化：可以使用布隆过滤器在插入数据的时候先检测一遍。
      2. 缩短域名：得到hash值之后可以转换成62进制字符串。6 位 62 进制数可表示 568 亿的数，应付长链转换绰绰有余
   2. 自增ID算法
      1. UUID
         1. UUID是一类算法的统称，具体有不同的实现。UUID的有点是每台机器可以独立产生ID，理论上保证不会重复，所以天然是分布式的，缺点是生成的ID太长，不仅占用内存，而且索引查询效率低。
      2. 多台mysql服务器，
         1. 假设用8台MySQL服务器协同工作，第一台MySQL初始值是1，每次自增8，第二台MySQL初始值是2，每次自增8，依次类推。前面用一个 round-robin load balancer 挡着，每来一个请求，由 round-robin balancer 随机地将请求发给8台MySQL中的任意一个，然后返回一个ID。
      3. Redis自增
      4. 雪花算法
         1. 最高位不用，永远为0，其余三组bit占位均可浮动，看具体的业务需求而定。默认情况下41bit的时间戳可以支持该算法使用到2082年，10bit的工作机器id可以支持1023台机器，序列号支持1毫秒产生4095个自增序列id。
3. 预防攻击
   1. 限制IP请求数


###  1.3. <a name='FAQ'></a>索引FAQ
`什么时候应该创建索引？`
- 主键
- 需要进行排序和范围检索的字段
- 需要进行检索的字段

`什么时候不应该创建索引？`
- 查询中很少使用的字段
- 区分度很低的字段，比如性别
- 类型是text、image或者二进制的字段
- 当修改性能远远大于检索性能时，不应该创建索引。因为修改性能和检索性能是矛盾的，


###  1.4. <a name='-1'></a>计数系统设计
主要功能要求根据业务需求直询指定的文章的计数统计（播放数，问读数，评论数等）
要求：支持实时更新各种计数，支持高并发查询，需要给出系统存储设计方案，对外输出接口设计；

**答案：**
- 方案一
  - 数据库+cache，数据量大之后可以采用分库分表，扩展性有限，但是开发运维成本低，性能方面通过cache优化，存在热点数据。
- 方案二
  - 使用Redis作为kv存储，查询效率够高，有资源问题，假设有10亿数据，如何保证以更低的成本满足该需求。
- 方案三
  - 自研counter模块，需要考感kv存储方案，value的设计，保证使用更少内存；还需要考虑的点：容灾备份机制；数据扩展问题；
- 方案四
  - 可能业务方经常新增计数需求，需要考虑counter服务的列扩展性，故设计的数据结构需要考虑列扩展问题；

数据库 + 缓存的方案是计数系统的初级阶段，完全可以支撑中小访问量和存储量的存储服务。如果你的项目还处在初级阶段，量级还不是很大，那么你一开始可以考虑使用这种方案。

通过对原生 Redis 组件的改造，我们可以极大地减小存储数据的内存开销。

使用 SSD+ 内存的方案可以最终解决存储计数数据的成本问题。这个方式适用于冷热数据明显的场景，你在使用时需要考虑如何将内存中的数据做换入换出。


###  1.5. <a name='Redis'></a>Redis提供了哪几种持久化方式
`AOF`

记录每次对服务器写的操作，当服务器重启的时候就会读取aof文件执行命令进行恢复数据，在aof文件大于64m的时候，会进行重写，重写后对每个key的命令只保留最新一条，进行体积压缩。

`RDB`

RDB持久化方式能够在指定的时间间隔能对你的数据进行快照存储


###  1.6. <a name='-1'></a>直播消息服务设计

直播间内的聊天、弹幕、礼物等消息，都是通过消息服务发送。
###  1.7. <a name='-1'></a>问题：
1. 设计一个消息服务，支持直播问内的用户进行聊天、发弹幕等操作
2. 衡量一个消息服务的核心指标有哪些？
3. 基于候选者的方案，如何监控和优化这些核心指标？
4. (深入）拉模型的分布式方案

**答案**

主要有推模型和拉模型

`推模型`

基于长链接，直播间内产生的消息，异步通过长链接发送给直播间内的其他观众

1. 长连接的架构（协议、鉴权、折容、重连、到达率等）
2. 消息放大问题如何解洪（比如一个有1万人的房间，任何一个人发的消息，都会产生1万个消息，相当于放大了1万倍）
   
    消息聚合、消息多級推送（类似CDN的方式）
3. 直播间用户列表怎么存储和优化

    推送消息时，需要房间内所有用户消息，对于观众比较多的房间，需要考虑数据分片、本地缓存等手段进行优化

`拉模型`

拉模型是把直播间的消息都存储在消息队列中，直播间内的用户通过前端轮询拉取消息

1. 房间的消息如何存储（由于消息有时效性，所以只需要存储最近一段时间的数据)
2. 轮询方式如何优化
3. 拉接口如何优化 (ocal cache等）


`核心指标`

消息每秒吞吐量、消息到达率、消息延时 （像稳定性这种，属于通用的基本指标）

`核心的优化方式（提供一些方式，其它的只要合理即可)`

监控方式：
1. 吞吐量（类似于 qps，打metrics等都可以）
2. 到达率：对于推模型，基本等价于长连接的到达率监控：对于拉模型，性价比较高的是只监控主播的（因为只有主播是全程在直播间的)
3. 延时：需要关注手机和服务端的时间不一致的问题


优化：
1. 吞吐量：批量发送、多级推送等
2. 到达率：一股推模型需要重点关注，主要是对于长连接的到达率优化，包含死连接检测等
3. 延时：一般拉模型需要重点关注，对于每次都拉到消息的房间，减少轮询间隔和长连接拉模式等。


###  1.8. <a name='-1'></a>秒杀系统设计

设计要点：
1. 可以支撑QPS 100万，能分析系统的瓶颈
2. 如何尽快把商品卖完，但不能超卖
3. 考察系统扩展性、容灾能力


`答案`

参考： 
- https://cloud.tencent.com/developer/article/1520361
- https://blog.51cto.com/u_13540373/5457725
- https://gongfukangee.github.io/2019/06/09/SecondsKill/


秒杀系统特点：
- 高性能：秒杀涉及大量的并发读和并发写，因此支持高并发访问这点非常关键
- 一致性：秒杀商品减库存的实现方式同样关键，有限数量的商品在同一时刻被很多倍的请求同时来减库存，在大并发更新的过程中都要保证数据的准确性。
- 高可用：秒杀时会在一瞬间涌入大量的流量，为了避免系统宕机，保证高可用，需要做好流量限制

优化思路：
- 后端优化：将请求尽量拦截在系统上游
  - 限流：屏蔽掉无用的流量，允许少部分流量走后端。假设现在库存为 10，有 1000 个购买请求，最终只有 10 个可以成功，99% 的请求都是无效请求
  - 削峰：秒杀请求在时间上高度集中于某一个时间点，瞬时流量容易压垮系统，因此需要对流量进行削峰处理，缓冲瞬时流量，尽量让服务器对资源进行平缓处理
  - 异步：将同步请求转换为异步请求，来提高并发量，本质也是削峰处理
  - 利用缓存：创建订单时，每次都需要先查询判断库存，只有少部分成功的请求才会创建订单，因此可以将商品信息放在缓存中，减少数据库查询
  - 负载均衡：利用 Nginx 等使用多个服务器并发处理请求，减少单个服务器压力
- 前端优化：
  - 限流：前端答题或验证码，来分散用户的请求
  - 禁止重复提交：限定每个用户发起一次秒杀后，需等待才可以发起另一次请求，从而减少用户的重复请求
  - 本地标记：用户成功秒杀到商品后，将提交按钮置灰，禁止用户再次提交请求
  - 动静分离：将前端静态数据直接缓存到离用户最近的地方，比如用户浏览器、CDN 或者服务端的缓存中
- 防作弊优化：
  - 隐藏秒杀接口：如果秒杀地址直接暴露，在秒杀开始前可能会被恶意用户来刷接口，因此需要在没到秒杀开始时间不能获取秒杀接口，只有秒杀开始了，才返回秒杀地址 url 和验证 MD5，用户拿到这两个数据才可以进行秒杀
  - 同一个账号多次发出请求：在前端优化的禁止重复提交可以进行优化；也可以使用 Redis 标志位，每个用户的所有请求都尝试在 Redis 中插入一个 userId_secondsKill 标志位，成功插入的才可以执行后续的秒杀逻辑，其他被过滤掉，执行完秒杀逻辑后，删除标志位
  - 多个账号一次性发出多个请求：一般这种请求都来自同一个 IP 地址，可以检测 IP 的请求频率，如果过于频繁则弹出一个验证码
  - 多个账号不同 IP 发起不同请求：这种一般都是僵尸账号，检测账号的活跃度或者等级等信息，来进行限制。比如微博抽奖，用 iphone 的年轻女性用户中奖几率更大。通过用户画像限制僵尸号无法参与秒杀或秒杀不能成功






##  2. <a name='-1'></a>算法